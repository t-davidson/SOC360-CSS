---
title: "Computational Social Science" 
subtitle: "Introduction to Natural Language Processing II"
author: Dr. Thomas Davidson
institute: Rutgers University
date: October 16, 2024
output:
    beamer_presentation:
      theme: "Szeged"
      colortheme: "beaver"
      fonttheme: "structurebold"
      toc: false
      incremental: false
header-includes:
  - \usepackage{multicol}
  - \usepackage{caption}
  - \usepackage{hyperref}
  - \captionsetup[figure]{font=scriptsize}
  - \captionsetup[figure]{labelformat=empty}
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
library("knitr")
library("formatR")

opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
opts_chunk$set(tidy = FALSE)

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})
```

# Plan
1. Course updates
2. TF-IDF weighting
3. Vector representations of texts
4. Cosine similarity

# Course updates
- Homework 2 due tonight
- Project proposals due next Wednesday
    - Complete quiz on Canvas

# Working with text
## Recap
- Introduction to Natural Language Processing
- Pre-processing texts
  - Tokenization, stemming, stop word removal
- The bag-of-words representation
  - N-grams

# Working with text
## Comparing documents
- The goal of today's lecture is to introduce methods for comparing documents
  - Re-weighting word counts to find distinctive words
  - Representing documents as vectors of word counts
  - Geometric interpretations of document vectors
  
# Working with text
## Limitations of word counts
- Word counts  alone are an imperfect measure for comparing documents
  - Some words occur in most documents, providing little information about the document (recall Zipf's law)
  - Similarly, some words are very rare, providing little generalizable insight
  - We want to find words that help distinguish between documents

# Working with text 
## Term-frequency inverse document-frequency (TF-IDF)  
- Term-frequency inverse document-frequency (TF-IDF) is a way to weight word counts ("term frequencies") to give higher weights to words that help distinguish between documents
  - Intuition: Adjust word counts to take into account how many documents a word appears in.

# Working with text
## Calculating term-frequency inverse document-frequency (TF-IDF)
- $N$ = number of documents in the corpus
- $tf_{t,d}$ = number of times term $t$ used in document $d$
- $df_{t}$ = number of documents containing term $t$
- $idf_{t} = log(\frac{N}{df_{t}})$ = log of fraction of all documents containing $t$
  - $\frac{N}{df_{t}}$ is larger for terms occurring in fewer documents
  - The logarithm is used to penalize very high values
  - If a word occurs in all documents $df_{t} = N$, thus $idf_{t} = log\frac{N}{N} = log(1) = 0$ .
- We then use these values to calculate $TF\-IDF_{t,d} = tf_{t,d}*idf_{t}$

# Working with text
## Loading data
Loading the word frequency objects created last lecture using `tidytext`.
```{r, echo=FALSE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
library(tidyverse)
library(ggplot2)
library(stringr)
library(tidytext)
library(gutenbergr)
#install.packages("tm") # Dependency for tidytext, required for cast_dtm

texts <- read_csv('../data/marxdurkheim.csv') # Original texts
words <- read_csv('../data/words.csv') # Word counts and totals
```


# Working with text
## Computing TF-IDF in `tidytext`
We can easily compute TF-IDF weights using `tidy.text` by using the word-count object we created last lecture.  Note the two document example is quite trivial. Many words have IDF scores equal to zero because they occur in both documents.
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
tidy.tfidf <- words %>% bind_tf_idf(word, title, n)
head(tidy.tfidf)
```

# Working with text
Take the stem "countri" for example (short for country, country's, countries). 
```{r, echo=FALSE, fig.width=6, fig.height=4,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
tidy.tfidf %>% filter(word == "countri")
```

# Working with text
The term "australia" has a relatively low term frequency but a higher IDF score, since it only occurs in *Elementary Forms*.
```{r, echo=FALSE, fig.width=6, fig.height=4,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
tidy.tfidf %>% filter(word == "australia")
```

# Working with text
In this case *all* words unique to one document will have the same IDF score, $\sim log(2/1)$.
```{r, echo=FALSE, fig.width=6, fig.height=4,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
head(tidy.tfidf %>% filter(round(idf,2) == 0.69))
```

# Working with text
```{r, echo=FALSE, fig.width=6, fig.height=4,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
tidy.tfidf %>% filter(title == "Communist Manifesto") %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = reorder(word, tf_idf)) %>%
  head(10) %>%
  ggplot(aes(tf_idf, word)) +
  geom_col(color='red') +
  labs(y = NULL, x='TF-IDF weight', title="10 stems with highest TF-IDF in The Communist Manifesto", caption="Stopwords removed+, stemmed")
```

# Working with text
```{r, echo=FALSE, fig.width=6, fig.height=4,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
tidy.tfidf %>% filter(title == "Elementary Forms") %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = reorder(word, tf_idf)) %>%
  head(10) %>%
  ggplot(aes(tf_idf, word)) +
  geom_col(color='blue') +
  labs(y = NULL, x='TF-IDF weight', title="10 stems with highest TF-IDF in Elementary Forms ", caption="Stopwords removed+, stemmed")
```

# Vector representations of texts
## The document-term matrix (DTM)
- A frequently used bag-of-words representation of a text corpus is the *Document-Term Matrix*:
  - Each row* is a document (a unit of text)
  - Each column is a term (word)
  - For a given DTM $X$, each cell $X_{i,j}$ indicates the number of times a term $i$ occurs in document $j$, $tf_{i,j}$.
    - This can be the raw term counts or TF-IDF weighted counts.
- Most cells are empty so it is usually stored as a sparse matrix to conserve memory.
  
\tiny \*Sometimes the rows and columns are reversed, resulting in a *Term-Document Matrix* or *TDM*

# Vector representations of texts
## Casting a `tidytext` object into a DTM
```{r, echo=TRUE,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
X <- texts %>% unnest_tokens(word, text) %>%
    anti_join(stop_words) %>% count(title, word) %>%
    cast_dtm(title, word, n)
print(X)
```
\tiny Note: This matrix is not weighted by TF-IDF, although we could apply the weights if desired.

# Vector representations of texts
## Viewing the DTM
The object created is a class unique to the `tidytext` package. We can inspect this to see what it contains.
```{r, echo=TRUE,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
class(X)
dim(X)
X$dimnames[1]
#X$dimnames[2] # prints all columns as a long list
X$dimnames[[2]][1:50] # first 50 columns
X$v[1:50] # first 50 values
```

# Vector representations of texts
## Viewing the DTM
The easiest way to see the actual DTM is to cast it to a matrix.
```{r, echo=TRUE,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
Xm <- as.matrix(X)
```

# Vector representations of texts
## Geometric interpretation
- Each text is a vector in N-dimensional space, where N is the total number of unique words (column of the DTM)
- Each word is a vector in D-dimensional space, where D is the number of documents (rows of the DTM)

\tiny See https://web.stanford.edu/~jurafsky/slp3/6.pdf for more details on the vector-space model

# Vector representations of texts
## Document vectors
```{r, out.width="70%",out.height="70%", fig.align="center"}
  include_graphics('../images/doc_vectors.png')
```
This example from Jurafsky and Martin shows a Term-Document Matrix (TDM) pertaining to four key words from four Shakespeare plays. The document vectors are highlighted in red.

# Vector representations of texts
## Document vectors
```{r, out.width="70%",out.height="70%", fig.align="center"}
  include_graphics('../images/vector_vis.png')
```
Here vectors for each play are plotted in two-dimensional space. The y- and x-axes indicate the number of times the words "battle" and "fool" appear in each play. Note how some vectors are closer than others and how they have different lengths.

# Vector representations of texts
## Word vectors
```{r, out.width="70%",out.height="70%", fig.align="center"}
  include_graphics('../images/word_vectors.png')
```
We could also treat the rows of this matrix as vector representations of each word. We will return to this idea next week.

# Cosine similarity
```{r, out.width="70%",out.height="70%", fig.align="center"}
  include_graphics('../images/cosine.png')
```

# Cosine similarity
```{r, out.width="70%",out.height="70%", fig.align="center"}
  include_graphics('../images/vector_vis_angles.png')
```

# Cosine similarity

$\vec{u}$ and $\vec{v}$ are vectors representing texts (e.g. rows from a DTM matrix). We can compute the cosine of the angle between these two vectors using the following formula:


$$ cos(\theta) = \frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\|\|\vec{v}\|} = \frac{\sum_{i}\vec{u_i} \vec{v_i}}{\sqrt{\sum_{i}\vec{u}_i^2} \sqrt{\sum_{i}\vec{v}_i^2}} $$

The value range from 0 (complete dissimilarity) to 1 (identical), since all values are non-negative.

# Cosine similarity
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
u <- c(1,2,3,4)
v <- c(0,1,0,1)

sum(u*v) / (sqrt(sum(u^2)) * sqrt(sum(v^2)))

# Same result using matrix multiplication
u %*% v / (sqrt(u %*% u) * sqrt(v %*% v))
```

# Cosine similarity
## Making a function
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
cosine.sim <- function(u,v) {
  numerator <- u %*% v
  denominator <- sqrt(u %*% u) * sqrt(v %*% v)
  return (numerator/denominator)
}

cosine.sim(u,v)
```



# Cosine similarity
## Cosine similarity between Marx and Durkheim
We can use the two columns of the DTM matrix defined above as arguments to the similarity function.
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
print(cosine.sim(Xm[1,], Xm[2,]))
```

# Cosine similarity
## Cosine similarity for a larger corpus
Let's consider another example with a slightly larger corpus of texts.
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
m <- gutenberg_metadata %>% 
    filter(author == "Shakespeare, William" & language == "en")
plays <- gutenberg_download(2235:2269)

plays <- plays %>% left_join(m, by = "gutenberg_id") %>%
    filter(gutenberg_id != 2240) # Removing a duplicate
```


# Cosine similarity
## From text to DTM
```{r, echo=FALSE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
plays.tidy <- plays %>% unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(title, word) %>% 
  group_by(word) %>%
  mutate(total = sum(n),
         play_count = n_distinct(title)) %>%
  ungroup() %>%
  filter(total >= 10 & play_count >= 2) %>% 
  filter(!str_detect(word, "^[0-9]+$")) %>%
  bind_tf_idf(word, title, n)
    
DTM <- plays.tidy %>% cast_dtm(title, word, tf) 
  
print(DTM)
dim(DTM)
```

# Cosine similarity
## Extracting TF-IDF matrix
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
DTMd <- as.matrix(DTM)
write.csv(DTMd %>% as.data.frame(), "../data/shakespeare.csv", row.names = T)

# Run line below if using tf-idf weights as
# some columns contain zeros and must be removed
#DTMd <- DTMd[,colSums(DTM) > 0]
```

# Cosine similarity
## Normalizing columns
We can simplify the cosine similarity calculation if we normalize each column by its length (the denominator in the above calculation.)
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize', warning = FALSE}
normalize <- function(v) {
  return (v/sqrt(v %*% v))
}

# Normalizing every column in the matrix
for (i in 1:dim(DTMd)[1]) {
  DTMd[i,] <- normalize(DTMd[i,])
}
```

# Cosine similarity
## Calculating cosine similarity using matrix multiplication
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
sims <- DTMd %*% t(DTMd)
```

# Cosine similarity
```{r, echo=FALSE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
library(reshape2)
library(viridis)
data <- melt(sims)
colnames(data) <- c("play_i", "play_j", "similarity")

ggplot(data, aes(x = play_i, y = play_j, fill = similarity)) + geom_tile() +
  scale_fill_gradient2() +
  scale_fill_viridis_c()+
  theme_minimal() + ylim(rev(levels(data$play_i))) + xlim(levels(data$play_j))
```

# Cosine similarity
```{r, echo=FALSE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
sims2 <- sims
diag(sims2) <- NA # Set diagonal values to NA

data <- melt(sims2)
colnames(data) <- c("play_i", "play_j", "similarity")

ggplot(data, aes(x = play_j, y = play_i, fill = similarity)) + geom_tile() +
  #scale_fill_gradient2() +
  scale_fill_viridis_c() +
  theme_minimal()  + labs (x = "", y = "", title = "Cosine similarity of Shakespeare's plays") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + ylim(rev(levels(data$play_i))) + xlim(levels(data$play_j))
```

# Cosine similarity
```{r, echo=FALSE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
ggplot(data, aes(x = play_j, y = play_i, fill = similarity)) + geom_tile() +
  scale_fill_viridis_c(option = "plasma", direction = -1) +
  theme_minimal()  + labs (x = "", y = "", title = "Cosine similarity of Shakespeare's plays") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + ylim(rev(levels(data$play_i))) + xlim(levels(data$play_j))
```

# Next week
- Word embeddings