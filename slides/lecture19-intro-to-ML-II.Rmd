---
title: "Computational Social Science" 
subtitle: "Supervised Machine Learning"
author: Dr. Thomas Davidson
institute: Rutgers University
date: March 28, 2024
output:
    beamer_presentation:
      theme: "Szeged"
      colortheme: "beaver"
      fonttheme: "structurebold"
      toc: false
      incremental: false
header-includes:
  - \usepackage{multicol}
  - \usepackage{caption}
  - \usepackage{hyperref}
  - \captionsetup[figure]{font=scriptsize}
  - \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
library("knitr")
library("formatR")

opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
opts_chunk$set(tidy = FALSE)

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})
```

# Plan

1.  Course updates
2.  Classification algorithms
3.  Intro to machine learning in R

# Course updates

## Homework

-   Homework 3 was due yesterday
-   Homework 4 on supervised machine learning released next week

# Recap

-   Supervised learning optimizes for predictive accuracy (focus on $\hat{Y}$ not $\hat{\beta}$)
-   Problems of over and under-fitting
    -   Out-of-sample validation and cross-validation
    -   Regularization
-   Evaluating model performance
    -   Precision, recall, F1, ROC/AUC

# Recap

-   Given some outcome $Y$ and a matrix of features $X$, we want to find a function $Y=f(X)$ that best predicts the outcome

# Recap

```{r, out.width="70%",out.height="60%", fig.align="center"}
include_graphics('../images/penguins.jpg')
```

# Recap
## Predicting penguins

-   $Y = 1$ if the bird is a penguin, otherwise $Y = 0$
-   $X$ is a matrix including information on birds including their diet, wingspan, coloring, locations, etc.
    -   Some of the information will be useful (e.g. ability to fly) but other information will be less meaningful (e.g. coloring)
-   Goal is to find $f(X)$ to predict whether a given bird is a penguin
-   The quality of the prediction will depend on both the information contained in $X$ and the properties of the function $f()$.

# Classification algorithms

-   Logistic regression
-   Support vector machines (SVM)
-   Decision trees and random forests
-   Neural networks
-   And many others

# Machine learning in R

## Tidymodels

-   `tidymodels` is a set of packages designed to use tidy principles to conduct machine-learning.

    -   See <https://www.tidymodels.org/packages/> for a list of packages.

    ```{r, out.width="50%",out.height="35%", fig.align="center"}
    include_graphics('../images/tidymodels.png')
    ```

    \tiny \centering Source: \href{https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/}{tidymodels tutorial.}

# Machine learning in R

## Loading `tidymodels`

The `tidymodels` package loads all of the sub-packages, as well as the `tidyverse` packages. We're going to be using a sample of data from the General Social Survey (GSS). The goal will be to predict whether a respondent has a college degree (or higher) as a function of their survey responses.

```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
library(tidyverse)
library(tidymodels)
data <- read_csv("../data/2018_gss_sample.csv")

table(data$degree)
```


# Machine learning in R
## Data cleaning
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
colnames(data)

data <- data %>%
  mutate(across(-c(age, sibs), as.factor))
```
    
    
# Machine learning in R

## Splitting data

We can use the `initial_split` command to create a train-test split, where 20% of the data are held-out for testing.

```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
set.seed(987123)
data_split <- initial_split(data, prop = 0.8)
print(data_split)
```

# Machine learning in R
## Viewing the traing data

```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
data_split %>% training() %>% head()
```

# Machine learning in R
## Pre-processing using `recipe`

We will use the `recipes` package to pre-process the data.
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
data_recipe <- training(data_split) %>%
  recipe(degree ~ .) %>%
  step_scale(all_numeric_predictors(), -all_outcomes()) %>%
  step_dummy(all_factor_predictors(), -all_outcomes()) %>%
  prep()

data_recipe
```

# Machine learning in R
## Extracting data from `recipe`

The previous chunk only applied these transformations to the training data. We want to also modify the test data so that they are the same dimensions. We can apply the `recipe` to the new data using the `bake` command. We also want to load the training data using the `juice` command. This extracts the data directly from the recipe.

```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
data_testing <- data_recipe %>% 
  bake(testing(data_split))

data_training <- juice(data_recipe)
```

# Machine learning in R
## Fitting a model

ML models in R exist across a range of different packages and `parsnip` gives them a standardized syntax. We define the model, choose the package (in this case `randomForest`), then use `fit` to train the model.
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
library(randomForest)
rf <-  rand_forest(trees = 1000, mode = "classification") %>%
  set_engine("randomForest") %>%
  fit(degree ~ ., data = data_training)
```

# Machine learning in R
## Making predictions (in-sample)
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
preds <- predict(rf, data_training)
train_preds <- bind_cols(data_training, preds) %>%
    select(degree, .pred_class)
head(train_preds)
```

# Machine learning in R
## Calculating metrics (in-sample)
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
precision <- train_preds %>% precision(truth=degree, estimate = .pred_class)
recall <- train_preds %>% recall(truth=degree, estimate = .pred_class)
print(bind_rows(precision, recall))
```

# Machine learning in R
## Making predictions (out-of-sample)
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
preds <- predict(rf, data_testing)
test_preds <- bind_cols(data_testing, preds) %>%
    select(degree, .pred_class)
```

# Machine learning in R
## Calculating metrics (out-of-sample)
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
precision <- test_preds %>% precision(truth=degree, estimate = .pred_class)
recall <- test_preds %>% recall(truth=degree, estimate = .pred_class)
print(bind_rows(precision, recall))
```

# Machine learning in R
## Calculating metrics: Predicted probabilities
We can also extract the predicted probabilities by adding an argument to the `predict` function.
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
probs <- rf %>%
  predict(data_testing, type = "prob") %>%
  bind_cols(data_testing)
head(probs %>% select(degree, .pred_0, .pred_1) %>% bind_cols(preds))
```

# Machine learning in R
## Calculating metrics: ROC
```{r, echo=FALSE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
probs %>% roc_curve(degree, .pred_0) %>% autoplot()
```

# Machine learning in R
## Calculating metrics: AUC
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
probs %>% roc_auc(degree, .pred_0)
```

# Machine learning in R
## Alternatives

-   Python has a more developed ML ecosystem than R.
    -   `scikit-learn` provides a suite of tools for most machine-learning tasks except deep-learning, which requires specialized libraries.

    ```{r, out.width="45%",out.height="40%", fig.align="center"}
    include_graphics('../images/scikit.png')
    ```

    \tiny \centering Source: \href{https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html}{scikit-learn documentation}. See this \href{https://www.r-bloggers.com/2020/04/how-to-run-pythons-scikit-learn-in-r-in-5-minutes/}{tutorial} for how to run `scikit-learn` using R.
    
# Machine learning in R
## Next week
-   Supervised machine learning to perform text classification
-   Cross-validation, parameter searches, and model comparison
-   Data quality and predictive performance
