---
title: "Computational Social Science" 
subtitle: "Agent-based Modeling / Tabular data and visualization"
author: Dr. Thomas Davidson
institute: Rutgers University
date: September 15, 2025
output:
    beamer_presentation:
      theme: "Szeged"
      colortheme: "beaver"
      fonttheme: "structurebold"
      toc: false
      incremental: false
header-includes:
  - \usepackage{multicol}
  - \usepackage{caption}
  - \usepackage{hyperref}
  - \captionsetup[figure]{font=scriptsize}
  - \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
library("knitr")
library("formatR")

opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
opts_chunk$set(tidy = FALSE)

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})
```

# Plan

- Recap
- Part I
    - Agent-based modeling
- Part II
    - Working with tabular data in R


# Recap
## Programming fundamentals
- Boolean logic
- If-else statements
- Loops
- Functions

# Part I
## Agent-based modeling

# What is agent-based modeling?
## Agent-based modeling and quantitative social science
- Most quantitative social science is *variable-centered*. We study the associations and interactions between variables using statistical techniques.
- For example, what is the association between college attendance and income for adults?
    - *Dependent variable*: Income
    - *Independent variable*: College attendance
    - *Controls*: Test scores, parental education, socioeconomic status, gender, etc.
    
# What is agent-based modeling?
## Agent-based modeling and quantitative social science
- Agent-based modeling is the study of "social life as interactions among adaptive agents who influence one another in response to the influence they receive." (Macy and Willer 2002)
  - Rather than interactions between variables, we consider *interactions between interdependent individuals*
  
# What is agent-based modeling?
## Agent-based modeling and quantitative social science  
- Often we are interested in the *emergent* properties of local interactions between agents and how they aggregate into system-level processes such as diffusion, polarization, and segregation
  - How do cultural tastes spread through social networks?
  - Why do people become polarized about politics?
  - What explains patterns of residential segregation?

# What is agent-based modeling?
## Key assumptions
- Macy and Willer 2002 outline four key assumptions that underpin many sociological agent-based models
  - Agents are *autonomous*
    - There is no system-wide coordination
  - Agents are *interdependent*
    - Agents respond to each other and to their environment
  - Agents follow *simple rules*
    - Simple local rules can generate global complexity
  - Agents are *adaptive* and *backwards looking*
    - Agents can alter their behavior through processes such as imitation and learning

# What is agent-based modeling?
## Craig Reynolds *Flocking behavior* (1987)
```{r, out.width="70%",out.height="70%", fig.align="center"}
include_graphics('../images/boids.png')
```
\tiny \centering Reynolds, Craig W. 1987. “Flocks, Herds and Schools: A Distributed Behavioral Model.” In *Proceedings of the 14th Annual Conference on Computer Graphics and Interactive Techniques*, 25–34.

# NetLogo and NetLogoWeb
## Flocking behavior in NetLogo
http://www.netlogoweb.org/launch#http://ccl.northwestern.edu/netlogo/models/models/Sample%20Models/Biology/Flocking.nlogo


# What is agent-based modeling?
## Thomas Schelling *Homophily and segregation*
```{r, out.width="70%",out.height="70%", fig.align="center"}
include_graphics('../images/shelling.png')
```
\tiny \centering Schelling, Thomas C. 1971. “Dynamic Models of Segregation.” *Journal of Mathematical Sociology* 1: 143–86.

# NetLogo
## Schelling's segregation model in NetLogo
http://www.netlogoweb.org/launch#http://ccl.northwestern.edu/netlogo/models/models/IABM%20Textbook/chapter%203/Segregation%20Extensions/Segregation%20Simple.nlogo

# What is agent-based modeling?
## DellaPosta, Shi, and Macy *Why Do Liberals Drink Lattes?* (2015)
```{r, out.width="70%",out.height="70%", fig.align="center"}
include_graphics('../images/dellaposta.png')
```
\tiny \centering DellaPosta, Daniel, Yongren Shi, and Michael Macy. 2015. “Why Do Liberals Drink Lattes?” *American Journal of Sociology* 120(5): 1473–1511.


# What is agent-based modeling?
## Why use it?
- Benefits
    - Test theories that are hard to evaluate using data
    - Dynamic and flexible
- Drawbacks
    - Works best with simple models, intractable with more complex simulations
    - Difficult to evaluate or benchmark

# Example: Schelling's segregation model

Copy the code at this link and try to run it as an R script in RStudio: \href{http://tinyurl.com/schellingR}{http://tinyurl.com/schellingR}

# Part II
## Tabular data and data visualization

# Tabular data
- Often we will be using intermediate objects like lists, vectors, and matrices in conjunction with these different programming techniques to produce tabular data

# Tabular data
## Data frames
A data frame is a tabular representation that has a similar structure to a list but allows us to organize the data more neatly.
```{r, echo=TRUE, , mysize=TRUE, size='\\footnotesize' }
data(iris) # The `data` function loads a built in dataset
head(iris)
iris$Sepal.Length[1] # explicitly call column name
iris[[1]][1] # reference column using index
```

# Tabular data
## The tidyverse
- The ``tidyverse`` is an R package that contains an entire suite of functions designed for such tasks including
  - ``dplyr`` contains functions for basic manipulation and merges and joins
  - ``tidyr`` implements functions for data cleaning and shape transformations
  - ``purr`` contains methods to easily map functions to lists and data frames.
- \href{https://www.rstudio.com/resources/cheatsheets/}{RStudio cheatsheets} is an excellent resource.
  
# Tabular data
## The tidyverse

```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
library(tidyverse)
tidyverse::tidyverse_packages()
```
Visit the tidyverse website for more information on the different packages \href{https://tidyverse.tidyverse.org/}{website}

# Tabular data
## Tibbles
A `tibble` is the `tidyverse` take on a data.frame. We can easily convert any `data.frame` into a `tibble`.
```{r, echo=TRUE, , mysize=TRUE, size='\\footnotesize' }
iris.t <- as_tibble(iris) # convert to tibble
class(iris.t)
```

# Tabular data
## Tibbles
Tibbles only show the first ten rows when printing (both look the same in RMarkdown, so we have to use the console to compare.) Tibbles also provide information on the type of each variable.
```{r, echo=TRUE, , mysize=TRUE, size='\\footnotesize' }
print(iris.t)
```

# Tabular data
## Tibbles
Tibbles also tend to provide more warnings when potential issues arise, so they should be less prone to errors than data frames.
```{r, echo=TRUE, , mysize=TRUE, size='\\footnotesize', warning=T }
iris$year # data.frame shows null
iris.t$year # tibble provides a warning
```

# Tabular data
## Reading data
We can read data from files or directly from the web using `readr`. Here we're reading in data from the *New York Times* state-level COVID-19 tracker. The `glimpse` command shows us a preview of the table. We can use `View` to open up the data in a new window.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv")
dim(c19)
glimpse(c19)
tail(c19)
```


# Tabular data
## Selecting columns
We can use the select command to select subsets of columns in the dataset.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 |> select(date, state, cases) # Select these columns
c19 <- c19 |> select(-fips) # select all except named column, replace dataset
```

# Tabular data
## Filtering
The `filter` command allows us to subset rows that meet one or more conditions.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 |> filter(cases > 1000) # conditional filtering
c19 |> filter(str_detect(state, "New")) # str_detect function from stringr package
c19 |> filter(str_detect(state, "New") & cases > 1000)
c19 |> filter(date >= as.Date("2023-03-23")) # using as.Date to case string to date
```

# Tabular data
## Filtering
The `filter` command allows us to subset rows that meet one or more conditions.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 |> filter(date >= as.Date("2023-03-23")) |> head(1) # using as.Date to case string to date
```

# Tabular data
## Sampling
We can also filter our dataset by taking a sample. This can be very useful for testing purposes.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 |> sample_n(2) # Randomly pick n rows
c19 |> sample_frac(0.0001) # Randomly a tiny proportion of rows
```

# Tabular data
## Slicing
The `slice` commands can be used to select ordered subsets of rows. Note how we can add the dataset as an argument rather than using a pipe.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
slice_max(c19, order_by = cases, n = 10) # Get the top n rows by a specified column
slice_min(c19, order_by = cases, n = 1, with_ties = T) # with_ties determines whether tied results are returned.
```

# Tabular data
## Making new columns using mutate
The `mutate` function allows us to generate new columns.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 <- c19 |> mutate(deaths_per_case = deaths / cases)
colnames(c19)
```

# Tabular data
## Mutate
Although these data are cumulative, we can recover the new cases and deaths each day by using the `lag` operator.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 <- c19 |> group_by(state) |> mutate(new_cases = cases - lag(cases),
                                          new_deaths = deaths - lag(deaths)) |>
    ungroup()
tail(c19 |> filter(state == "Oregon"))
```

# Tabular data
## Summarizing
We can use `summarize` to create statistical summaries of the data. Like `mutate`, we define a new variable within `summarize` to capture a defined summary.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
# Summarize specific variables
c19 |> summarise(mean_deaths = mean(deaths), 
                  median_deaths = median(deaths),
                  max_deaths = max(deaths))
```

# Tabular data
## Summarizing
The `summarize_all` command takes a summary function (e.g. mean, min, max) and applies it to all columns. This can be useful if there are lots of variables. See documentation for other variants of summarize. Note that the mean is undefined for non-numeric columns AND columns with missing data.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 |> summarize_all(mean) |> select(state,cases,new_cases) # Map a summary function to all valid columns
```

# Tabular data
## Summarizing
We can *impute* missing data to get an estimate of the mean. In this case, values are missing for early rows where the lag operator was not defined. Missing `new_cases` or `new_deaths` will be set to zero using `replace_na`.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 <- c19 |> replace_na(list(new_cases = 0, new_deaths = 0))
c19 |> summarize_all(mean) |> select(state, cases, new_cases) # Map a summary function to all valid columns
```

# Tabular data
## Grouping
Often we want to group our data before summarizing. What does this example tell us?
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 |> group_by(state) |> summarise(mean(deaths_per_case)) |> ungroup()
```

# Tabular data
## Grouping
Sometimes we might want to create a group-level variable then revert back to the original dataset. We can do this using the `ungroup` command. What does this new column represent?
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 |> group_by(date) |> mutate(daily_mean = mean(new_cases)) |> ungroup() |>
    select(date, state, daily_mean) |> tail()
```

# Tabular data
## Joins
We often want to join together different datasets. There are four main types of joins:
```{r, out.width="70%",out.height="70%", fig.align="center", echo = F}
include_graphics('../images/joins.png')
```


# Tabular data
## Joins
The `left_join` is the most commonly used type of join. We keep all rows in our left dataset and the rows on the right dataset with valid matches. Here we're download a dataset about state governors and joining it on state. The `by` argument defines the columns we should join on.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
gov <- read_csv("https://raw.githubusercontent.com/OpenGovDataMirror/F_CivilServiceUSA_us-governors/master/us-governors/data/us-governors.csv")
gov <- gov |> select(state_name, party) # just select two columns

c19 <- c19 |> left_join(gov, by = c("state" = "state_name")) # We can pipe c19 into the left_join function.
```


# Tabular data
## Joining
Let's consider another example to get state-level population data. In this case, we're reading an Excel file from the Census bureau so we have to do a little more processing to load the file.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
library(readxl)
census <- "https://www2.census.gov/programs-surveys/popest/tables/2010-2019/state/totals/nst-est2019-01.xlsx"
# read_excel function from readxl does not currently handle files from the web, so we need to tell R to store as a temporary file
tmp <- tempfile(fileext = ".xlsx")
httr::GET(url = census, httr::write_disk(tmp))
pop <- read_excel(tmp)
```

# Tabular data
## Joining
These data are a little messier. We need to do a bit of cleaning up.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
pop.states <- pop[9:61, c(1,13)] # Selecting relevant rows and columns
colnames(pop.states) <- c("state", "pop") # Renaming
pop.states <- pop.states |>  # Standardizing
  mutate(state = str_replace(state, ".", "")) |>
  drop_na()
```

# Tabular data
## Joining
Now we can join our new column to the dataset. Finally, we drop rows that do not have a governor (`party` column is missing).
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 <- c19 |> left_join(pop.states, by = "state")
c19 <- c19 |> drop_na(party) # Dropping any row not considered a state
length(unique(c19$state)) # Verifying the correct number of states
```

# Data visualization
## ggplot2
The `ggplot2` library is loaded as part of the tidyverse. It can produce may different styles of plots with a simple, tidy syntax. Let's consider a basic example.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
ggplot(c19, # data
        aes(x = cases)) + # aesthetic mapping
    geom_histogram() # plot type
```

# Data visualization
## ggplot2
The previous histogram wasn't very informative because it doesn't show the trends over time. A better option would be to plot the cases over time.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
ggplot(c19, # data
       aes(x = date, y= cases)) + # aesthetic mapping
    geom_point() # plot type
```

# Data visualization
## ggplot2
We can see that the points above are lines, since we have daily measures for each state. Let's examine the linear trend by plotting the line of best fit to the data points.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
ggplot(c19, # data
       aes(x = date, y= cases)) + # aesthetic mapping
    geom_point() +
    geom_smooth(method='lm', se = F) # plot type
```

# Data visualization
## ggplot2
The previous line is not too informative due to variation among states. We can easily break it out by state by adding a `group` parameter. Now each state has a separate line fitted.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
ggplot(c19, # data
       aes(x = date, y= cases, group=state)) + # aesthetic mapping
    geom_smooth(method='lm', se = F) # plot type
```

# Data visualization
## ggplot2
We can also fit a smoothed line to better capture the trends.
```{r, echo=TRUE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
ggplot(c19, # data
       aes(x = date, y= cases, group=state)) + # aesthetic mapping
    geom_smooth(method='loess') # plot type
```

# Data visualization
## ggplot2
The color parameter allows us to assign a different color to each line. Note how things get a little difficult to read now.
```{r, echo=FALSE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
ggplot(c19, # data
       aes(x = date, y= cases, group=state, color=state)) + # aesthetic mapping
    geom_smooth(method='loess') # plot type
```

# Data visualization
## ggplot2
We can easily group by other variables.
```{r, echo=FALSE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
ggplot(c19, # data
       aes(x = date, y= cases, group=state, color=party)) + # aesthetic mapping
    geom_smooth(method='loess') # plot type
```

# Data visualization
## ggplot2
Why might the previous plot be misleading? Is there a better way to look at how cases vary by partisanship of the governor? Note: The plot is now stored as an object `p` before plotting. This allows us to modify it later on.
```{r, echo=FALSE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 <- c19 |> mutate(case_rate = cases / pop)
p <- ggplot(c19, # data
       aes(x = date, y= case_rate, group=state, color=party)) + # aesthetic mapping
    geom_smooth(method='loess') # plot type
p
```

# Data visualization
## ggplot2
Now we have a plot, let's make it look a bit nicer. We can easily add labels and modify the axes.
```{r, echo=FALSE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
p + # previous plot
    theme_minimal() + # themes change the overall look of a plot
    labs(x = "", y = "Infection rate", title = "Cumulative COVID-19 cases per capita by governor type, 2020-2023",
         color = "Governor affiliation", caption = "COVID-19 data from the New York Times. Updated March 23, 2023.") + 
    theme(axis.text.x = element_text(angle = 90))
```

# Data visualization
## ggplot2
We can easily modify this code to look at the data in a different way.
```{r, echo=FALSE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19 <- c19 |> mutate(daily_case_rate = new_cases / pop)
ggplot(c19, # data
       aes(x = date, y= daily_case_rate, group=party, color=party)) + # aesthetic mapping
    geom_smooth(method='lm') + # plot type
theme_minimal() + # themes change the overall look of a plot
    labs(x = "", y = "Infection rate", title = "Cumulative COVID-19 cases per capita by governor type, 2020-2023",
         color = "Governor affiliation", caption = "COVID-19 data from the New York Times. Updated March 23, 2023.") +  
    theme(axis.text.x = element_text(angle = 90))
```

# Data visualization
## ggplot2
Based on these results, we might want to go back to a non-linear fit and allow each state to have its own line to better see the trends.
```{r, echo=FALSE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
ggplot(c19, # data
       aes(x = date, y= daily_case_rate, group=state, color=party)) + # aesthetic mapping
    geom_smooth(method='loess') + # plot type
theme_minimal() + # themes change the overall look of a plot
    labs(x = "", y = "Daily infection rate", title = "Daily COVID-19 cases per capita by governor type, 2020-2023",
         color = "Governor affiliation", caption = "COVID-19 data from the New York Times. Updated March 23, 2023.") + 
    theme(axis.text.x = element_text(angle = 90))
```

# Data visualization
## ggplot2 maps
The `ggplot` package can be used to produce many different types of visualizations. For example, we can use it to produce maps. Here we load the package `maps` to get the shapefile for each state. The example 
```{r, echo=FALSE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
#install.packages("maps")
require(maps)
us_states <- map_data("state")

# We can plot an empty map
ggplot(data = us_states,
            mapping = aes(x = long, y = lat,
                          group = group)) + 
  geom_polygon(fill = "white", color = "black") + theme_minimal()
```
\tiny The code for this example is based on \href{https://socviz.co/maps.html}{Chapter 7} of Kieran Healy's *Data Visualization*


# Data visualization
## ggplot2
We have to merge our data with the shapefile in order to plot it on the map.
```{r, echo=FALSE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
c19.map <- c19 |> filter(date == as.Date("2023-03-22")) |> # select date
    mutate(state_lower = tolower(state)) |> # set to lowercase
    left_join(us_states, by = c("state_lower" = "region"))

p <- ggplot(data = c19.map,
            aes(x = long, y = lat,
                group = group, fill = party))

p + geom_polygon(color = "gray90", size = 0.1) +
    coord_map(projection = "albers", lat0 = 39, lat1 = 45) 
```

# Data visualization
## ggplot2
Let's try to do something more interesting.
```{r, echo=FALSE, tidy=TRUE, mysize=TRUE, size='\\footnotesize'}
#install.packages("ggthemes")
require(ggthemes)

c19.map <- c19.map |> mutate(cases_per_100k = new_cases / (pop/100000)) # a more interpretable metric

p <- ggplot(data = c19.map,
            aes(x = long, y = lat,
                group = group, fill = cases_per_100k))

p + geom_polygon(color = "gray90", size = 0.1) +
    coord_map(projection = "albers", lat0 = 39, lat1 = 45) + 
  scale_fill_gradient2(low = "blue", # Determines the color scale
                                mid = scales::muted("purple"),
                                high = "red") +
  theme_map() + # A theme for making maps
  labs(title = "COVID-19 new infection rate, March 23 2023", 
                     caption = "COVID-19 data from the New York Times. \n 2019 population estimates from the Census Bureau.",
                     fill = "Infections per \n 100k population")
```

# Mini homework (due before next class)
Follow the instructions on the course website to set up Github with RStudio \url{https://github.com/t-davidson/SOC360-CSS/} (click `github_setup.md`)

1. Register for a Github account
2. Install Git
3. Sync your Github account with RStudio

# Next lecture
- File management
- Github


